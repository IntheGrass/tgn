`# 数据处理
## 数据集预处理
1. 数据集的每篇边按照timestamp进行排列
2. 节点与边的id从1开始计数
3. 节点与边的特征向量在npy文件存储

## 数据集读取
1. 根据比例[0.70, 0.85]对应的timestamp: [val_time, test_time]分割train,val和test集。
2. 测试节点（test_node_set）：将timestamp大于**val_time**的边的u,i均作为测试节点
3. 测试新节点（new_test_node_set）：从测试节点集中随机选取1/10的节点，该数据用于评估归纳任务。因此，对于该节点集，需要**删除训练图中所有相关的边**。
4. 训练图（train_data）：由timestamp <= val_time的边组成，且删除new_test_node_set相关的边
5. 验证边（）
6. 总结：将边数据划分为三个部分，使用train_mask（还需要排除新节点相关的边）, val_mask, test_mask区分。其中val_mask与test_mask均包含一个new_node边子集，用于归纳任务。
    两个new_node边节点是否相交取决于参数，但总的new_node数量占总test_node_set数目的1/10。`

## 训练细节
1. 无邻居节点时: 将设置一个0节点作为其邻居

## Memory模块
1. Memory模块包含三个部分: memory, last_update, message
   * memory为一个尺寸为(n_nodes, memory_dim)的pytorch张量，初始化为0张量
   * last_update保留上次更新时，每个源节点的最新的边的timestamp
   * message为一个dict，保存上一个batch中
2. message更新：
   * 在每一次batch训练结束后更新，位于`get_raw_messages()`方法中
   * 原始的message单位消息包含两个部分[features, timestamp], timestamp为对应边的时间戳，features由如下特征拼接而成：
     * source_memory, 
     * destination_memory, 
     * edge_features, 
     * source_time_delta_encoding(当前边的timestamp与src_node对应的last_update的timestamp差值的时间编码)
   * 每个节点id在message中对应一个list，按照边/时间顺序排列
   * 更新消息时按无向边处理，即一条边的源节点和目标节点要分别生成一个消息。
   * 更新时，message会经过聚合以及一个GRU的memory_updater，将消息的特征维度转化为与memory_dim相等
   * memory更新后会将

# 整体流程
1. 